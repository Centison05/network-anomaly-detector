{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (2830743, 80)\n",
      "Label                     1.000000\n",
      "Bwd Packet Length Std     0.510216\n",
      "Bwd Packet Length Max     0.492007\n",
      "Bwd Packet Length Mean    0.484189\n",
      "Avg Bwd Segment Size      0.484189\n",
      "Packet Length Std         0.470252\n",
      "Max Packet Length         0.454054\n",
      "Packet Length Variance    0.453847\n",
      "Fwd IAT Std               0.422755\n",
      "Packet Length Mean        0.414059\n",
      "Average Packet Size       0.413037\n",
      "Idle Max                  0.394220\n",
      "Idle Mean                 0.390470\n",
      "Flow IAT Max              0.388666\n",
      "Fwd IAT Max               0.388642\n",
      "Idle Min                  0.380651\n",
      "Flow IAT Std              0.336720\n",
      "Fwd IAT Total             0.215468\n",
      "Flow Duration             0.213864\n",
      "FIN Flag Count            0.188632\n",
      "Name: Label, dtype: float64\n",
      "Columns identical / near-identical to Label: []\n",
      "Highly correlated with Label: ['Bwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Avg Bwd Segment Size', 'Packet Length Std', 'Max Packet Length', 'Packet Length Variance', 'Fwd IAT Std', 'Packet Length Mean', 'Average Packet Size', 'Idle Max', 'Idle Mean', 'Flow IAT Max', 'Fwd IAT Max', 'Idle Min', 'Flow IAT Std', 'Min Packet Length']\n",
      "Accuracy: 0.9989119472082437\n",
      "ROC-AUC: 0.9999471214051008\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    454620\n",
      "           1       1.00      1.00      1.00    111529\n",
      "\n",
      "    accuracy                           1.00    566149\n",
      "   macro avg       1.00      1.00      1.00    566149\n",
      "weighted avg       1.00      1.00      1.00    566149\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[454102    518]\n",
      " [    98 111431]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# --- 1ï¸âƒ£ Load Fresh Data ---\n",
    "data_path = r\"C:\\Users\\cmhub\\Desktop\\network-anomaly-detector-starter\\data\\MachineLearningCSV\\MachineLearningCVE\\CICIDS2017_combined.csv\"\n",
    "full_df = pd.read_csv(data_path)\n",
    "print(\"Loaded dataset:\", full_df.shape)\n",
    "\n",
    "# --- 2ï¸âƒ£ Shuffle to avoid sequential label blocks ---\n",
    "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# --- 3ï¸âƒ£ Identify obvious duplicates / extra label columns ---\n",
    "extra_labels = [c for c in full_df.columns if 'label' in c.lower() or 'attack' in c.lower()]\n",
    "print(\"Possible duplicate label columns:\", extra_labels)\n",
    "\n",
    "# --- 4ï¸âƒ£ Drop all known leak-prone or identifier columns ---\n",
    "known_leaks = [\n",
    "    # IDs / metadata\n",
    "    'Day', 'Timestamp', 'Flow ID', 'Source IP', 'Destination IP',\n",
    "    # Post-hoc rates and byte metrics\n",
    "    'Flow Bytes/s', 'Flow Packets/s',\n",
    "    # Packet-length and header-length stats\n",
    "    'Bwd Packet Length Std','Bwd Packet Length Max','Bwd Packet Length Mean',\n",
    "    'Avg Bwd Segment Size','Packet Length Std','Max Packet Length',\n",
    "    'Packet Length Variance','Fwd IAT Std','Packet Length Mean',\n",
    "    'Average Packet Size','Idle Max','Idle Mean','Flow IAT Max',\n",
    "    'Fwd IAT Max','Idle Min','Flow IAT Std','Min Packet Length',\n",
    "    'Fwd Header Length','Bwd Header Length',\n",
    "    # Bulk / flag features often deterministic for attacks\n",
    "    'Fwd Avg Bytes/Bulk','Bwd Avg Bytes/Bulk','Fwd Avg Packets/Bulk',\n",
    "    'Bwd Avg Packets/Bulk','Fwd Avg Bulk Rate','Bwd Avg Bulk Rate',\n",
    "    'URG Flag Count','PSH Flag Count','ECE Flag Count','RST Flag Count'\n",
    "]\n",
    "drop_cols = list(set(extra_labels + known_leaks + ['Label']))  # unique list\n",
    "X = full_df.drop(columns=drop_cols, errors='ignore').copy()\n",
    "y = full_df['Label'].copy()\n",
    "\n",
    "# --- 5ï¸âƒ£ Quick correlation check (should now be low) ---\n",
    "corrs = full_df.drop(columns=drop_cols, errors='ignore').corr(numeric_only=True)['Label'].abs().sort_values(ascending=False)\n",
    "print(\"\\nTop remaining correlations with Label:\\n\", corrs.head(10))\n",
    "\n",
    "# --- 6ï¸âƒ£ Basic numeric sanitization ---\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.clip(lower=-1e10, upper=1e10)\n",
    "\n",
    "# --- 7ï¸âƒ£ Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "assert not any(X_train.index.isin(X_test.index)), \"Train/test overlap!\"\n",
    "\n",
    "# --- 8ï¸âƒ£ Build pipeline ---\n",
    "pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=2,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 9ï¸âƒ£ Train & evaluate ---\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)\n",
    "proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, proba))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "\n",
    "# --- ðŸ”Ÿ Optional sanity check (random labels) ---\n",
    "from sklearn.utils import shuffle\n",
    "y_perm = np.random.permutation(y_train)\n",
    "pipe.fit(X_train, y_perm)\n",
    "print(\"\\nSanity accuracy (should be ~0.5):\", pipe.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ecc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#Hyperparameter grid for RandomizedSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [200, 300, 500], \n",
    "              'max_depth': [10, 20, None],\n",
    "              'min_samples_split': [2, 4, 6],\n",
    "              'min_samples_leaf': [1, 2, 3]}\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42)\n",
    "'''\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=2, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n",
    "'''\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,       # tries 20 random combos\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rand_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", rand_search.best_params_)\n",
    "print(\"Best F1 Score:\", rand_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
      "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
      "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
      "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
      "       'Fwd Packet Length Std'],\n",
      "      dtype='object')\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(X.columns[:10])\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c2a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rand_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Feature Importance Analysis \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m \u001b[43mrand_search\u001b[49m\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      3\u001b[0m importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(best_rf\u001b[38;5;241m.\u001b[39mfeature_importances_, index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rand_search' is not defined"
     ]
    }
   ],
   "source": [
    "#Feature Importance Analysis \n",
    "best_rf = rand_search.best_estimator_\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "importances.sort_values().tail(15).plot(kind='barh', color=\"steelblue\")\n",
    "plt.title(\"Top 15 Important Features for Anomaly Detection\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
